{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ecca20",
   "metadata": {},
   "source": [
    "# HW07 — Unsupervised Clustering (KMeans + DBSCAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34af29e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T10:43:27.075142Z",
     "iopub.status.busy": "2026-01-21T10:43:27.074780Z",
     "iopub.status.idle": "2026-01-21T10:43:31.191386Z",
     "shell.execute_reply": "2026-01-21T10:43:31.190627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: ['S07-hw-dataset-01.csv', 'S07-hw-dataset-02.csv', 'S07-hw-dataset-03.csv']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "BASE = Path('.')\n",
    "DATA_DIR = BASE / 'data'\n",
    "ART_DIR = BASE / 'artifacts'\n",
    "FIG_DIR = ART_DIR / 'figures'\n",
    "LABELS_DIR = ART_DIR / 'labels'\n",
    "\n",
    "for d in [DATA_DIR, ART_DIR, FIG_DIR, LABELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASETS = [\n",
    "    'S07-hw-dataset-01.csv',\n",
    "    'S07-hw-dataset-02.csv',\n",
    "    'S07-hw-dataset-03.csv',\n",
    "]\n",
    "print('Datasets:', DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefcb2b2",
   "metadata": {},
   "source": [
    "## Утилиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a20ea67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T10:43:31.194379Z",
     "iopub.status.busy": "2026-01-21T10:43:31.194031Z",
     "iopub.status.idle": "2026-01-21T10:43:31.208019Z",
     "shell.execute_reply": "2026-01-21T10:43:31.207443Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(path: Path):\n",
    "    df = pd.read_csv(path)\n",
    "    if 'sample_id' not in df.columns:\n",
    "        raise ValueError('Ожидался столбец sample_id')\n",
    "    sample_id = df['sample_id'].copy()\n",
    "    X = df.drop(columns=['sample_id'])\n",
    "    return df, sample_id, X\n",
    "\n",
    "def internal_metrics(X_scaled: np.ndarray, labels: np.ndarray, noise_label: int = -1):\n",
    "    labels = np.asarray(labels)\n",
    "    has_noise = np.any(labels == noise_label)\n",
    "    noise_fraction = float(np.mean(labels == noise_label)) if has_noise else 0.0\n",
    "\n",
    "    if has_noise:\n",
    "        mask = labels != noise_label\n",
    "        X_eval = X_scaled[mask]\n",
    "        labels_eval = labels[mask]\n",
    "    else:\n",
    "        X_eval = X_scaled\n",
    "        labels_eval = labels\n",
    "\n",
    "    n_clusters = len(set(labels_eval.tolist()))\n",
    "    if n_clusters < 2:\n",
    "        return {\n",
    "            'silhouette': float('nan'),\n",
    "            'davies_bouldin': float('nan'),\n",
    "            'calinski_harabasz': float('nan'),\n",
    "            'noise_fraction': noise_fraction,\n",
    "            'n_clusters_eval': int(n_clusters),\n",
    "            'n_points_eval': int(len(X_eval)),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'silhouette': float(silhouette_score(X_eval, labels_eval)),\n",
    "        'davies_bouldin': float(davies_bouldin_score(X_eval, labels_eval)),\n",
    "        'calinski_harabasz': float(calinski_harabasz_score(X_eval, labels_eval)),\n",
    "        'noise_fraction': noise_fraction,\n",
    "        'n_clusters_eval': int(n_clusters),\n",
    "        'n_points_eval': int(len(X_eval)),\n",
    "    }\n",
    "\n",
    "def pca_scatter(X_scaled: np.ndarray, labels: np.ndarray, title: str, outpath: Path):\n",
    "    Z = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(X_scaled)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(Z[:,0], Z[:,1], c=labels, s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def plot_curve(xs, ys, title, xlabel, ylabel, outpath: Path):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(xs, ys, marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def k_distance_eps_candidates(X_scaled: np.ndarray, k: int = 5, n_candidates: int = 7):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(X_scaled)\n",
    "    dists, _ = nbrs.kneighbors(X_scaled)\n",
    "    kth = np.sort(dists[:, -1])\n",
    "    percentiles = np.linspace(15, 85, n_candidates)\n",
    "    eps = np.unique(np.round(np.percentile(kth, percentiles), 3))\n",
    "    return eps.tolist(), kth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167c44c",
   "metadata": {},
   "source": [
    "## Эксперимент: KMeans (k=2..12) и DBSCAN (eps, min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f2b81d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T10:43:31.210119Z",
     "iopub.status.busy": "2026-01-21T10:43:31.209925Z",
     "iopub.status.idle": "2026-01-21T10:44:39.431058Z",
     "shell.execute_reply": "2026-01-21T10:44:39.430755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Artifacts: [PosixPath('artifacts/labels'), PosixPath('artifacts/best_configs.json'), PosixPath('artifacts/figures'), PosixPath('artifacts/labels_S07-hw-dataset-01.csv'), PosixPath('artifacts/labels_S07-hw-dataset-02.csv'), PosixPath('artifacts/labels_S07-hw-dataset-03.csv'), PosixPath('artifacts/metrics_summary.json')]\n",
      "Labels: [PosixPath('artifacts/labels/labels_S07-hw-dataset-01.csv'), PosixPath('artifacts/labels/labels_S07-hw-dataset-02.csv'), PosixPath('artifacts/labels/labels_S07-hw-dataset-03.csv')]\n"
     ]
    }
   ],
   "source": [
    "K_RANGE = list(range(2, 13))\n",
    "MIN_SAMPLES_GRID = [3, 5, 8, 12]\n",
    "\n",
    "metrics_summary = {}\n",
    "best_configs = {}\n",
    "\n",
    "for fname in DATASETS:\n",
    "    path = DATA_DIR / fname\n",
    "    df_raw, sample_id, X_df = load_dataset(path)\n",
    "\n",
    "    # --- ЯВНЫЙ препроцессинг (обязательный для distance-based)\n",
    "    # Поддерживаем и числовые, и категориальные признаки (на этих датасетах категориальных нет, но код готов под dataset-04).\n",
    "    num_cols = X_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in X_df.columns if c not in num_cols]\n",
    "\n",
    "    numeric_pipe = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    categorical_pipe = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_pipe, num_cols),\n",
    "            ('cat', categorical_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    Xs = preprocessor.fit_transform(X_df)\n",
    "    Xs = np.asarray(Xs)  # на всякий случай приводим к ndarray\n",
    "\n",
    "    ds_key = fname.replace('.csv','')\n",
    "    metrics_summary[ds_key] = {}\n",
    "\n",
    "    # --- KMeans\n",
    "    km_sils = []\n",
    "    best_k = None\n",
    "    best_km_labels = None\n",
    "    best_km_metrics = None\n",
    "\n",
    "    for k in K_RANGE:\n",
    "        km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=20)\n",
    "        labels = km.fit_predict(Xs)\n",
    "        m = internal_metrics(Xs, labels)\n",
    "        metrics_summary[ds_key][f'KMeans(k={k})'] = m\n",
    "        km_sils.append(m['silhouette'])\n",
    "        if not math.isnan(m['silhouette']) and (best_km_metrics is None or m['silhouette'] > best_km_metrics['silhouette']):\n",
    "            best_k = k\n",
    "            best_km_labels = labels\n",
    "            best_km_metrics = m\n",
    "\n",
    "    plot_curve(K_RANGE, km_sils,\n",
    "              title=f'{ds_key}: KMeans silhouette vs k',\n",
    "              xlabel='k', ylabel='silhouette',\n",
    "              outpath=FIG_DIR / f'{ds_key}_kmeans_silhouette_vs_k.png')\n",
    "\n",
    "    # --- DBSCAN\n",
    "    eps_candidates, kth = k_distance_eps_candidates(Xs, k=5, n_candidates=7)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(np.arange(len(kth)), kth, marker='.', linewidth=1)\n",
    "    plt.title(f'{ds_key}: k-distance plot (k=5)')\n",
    "    plt.xlabel('points sorted'); plt.ylabel('5-NN distance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f'{ds_key}_k_distance_plot.png', dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    db_best_metrics = None\n",
    "    db_best_labels = None\n",
    "    db_best_params = None\n",
    "\n",
    "    for eps in eps_candidates:\n",
    "        for ms in MIN_SAMPLES_GRID:\n",
    "            db = DBSCAN(eps=eps, min_samples=ms, n_jobs=1)\n",
    "            labels = db.fit_predict(Xs)\n",
    "            m = internal_metrics(Xs, labels, noise_label=-1)\n",
    "            metrics_summary[ds_key][f'DBSCAN(eps={eps},min_samples={ms})'] = m\n",
    "            if math.isnan(m['silhouette']):\n",
    "                continue\n",
    "            if (db_best_metrics is None\n",
    "                or m['silhouette'] > db_best_metrics['silhouette'] + 1e-12\n",
    "                or (abs(m['silhouette'] - db_best_metrics['silhouette']) <= 1e-12 and m['noise_fraction'] < db_best_metrics['noise_fraction'])):\n",
    "                db_best_metrics = m\n",
    "                db_best_labels = labels\n",
    "                db_best_params = {'eps': float(eps), 'min_samples': int(ms)}\n",
    "\n",
    "    # --- Выбор лучшего\n",
    "    chosen_method = 'KMeans'\n",
    "    chosen_params = {'k': int(best_k), 'random_state': RANDOM_STATE, 'n_init': 20}\n",
    "    chosen_labels = best_km_labels\n",
    "    chosen_metrics = best_km_metrics\n",
    "\n",
    "    if db_best_metrics is not None:\n",
    "        if math.isnan(best_km_metrics['silhouette']) or db_best_metrics['silhouette'] > best_km_metrics['silhouette'] + 0.02:\n",
    "            chosen_method = 'DBSCAN'\n",
    "            chosen_params = db_best_params\n",
    "            chosen_labels = db_best_labels\n",
    "            chosen_metrics = db_best_metrics\n",
    "\n",
    "    best_configs[ds_key] = {\n",
    "        'chosen_method': chosen_method,\n",
    "        'params': chosen_params,\n",
    "        'selection_criterion': 'maximize silhouette (DBSCAN: on non-noise points), choose DBSCAN if silhouette improves by > 0.02',\n",
    "        'metrics': chosen_metrics,\n",
    "        'dbscan_eps_candidates_from_k_distance_percentiles': eps_candidates,\n",
    "    }\n",
    "\n",
    "    # PCA scatter for chosen\n",
    "    pca_scatter(Xs, chosen_labels, f'{ds_key}: best {chosen_method} PCA(2D)', FIG_DIR / f'{ds_key}_best_pca2d.png')\n",
    "\n",
    "    # labels в artifacts/labels/\n",
    "    pd.DataFrame({'sample_id': sample_id, 'cluster_label': chosen_labels}).to_csv(LABELS_DIR / f'labels_{ds_key}.csv', index=False)\n",
    "\n",
    "# Сохраняем JSON\n",
    "(ART_DIR / 'metrics_summary.json').write_text(json.dumps(metrics_summary, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "(ART_DIR / 'best_configs.json').write_text(json.dumps(best_configs, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "\n",
    "print('Done!')\n",
    "print('Artifacts:', list(ART_DIR.iterdir()))\n",
    "print('Labels:', list(LABELS_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38610e90",
   "metadata": {},
   "source": [
    "## Итоговая таблица по лучшим решениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defed0b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T10:44:39.432461Z",
     "iopub.status.busy": "2026-01-21T10:44:39.432338Z",
     "iopub.status.idle": "2026-01-21T10:44:39.441341Z",
     "shell.execute_reply": "2026-01-21T10:44:39.441078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>method</th>\n",
       "      <th>params</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>davies_bouldin</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>noise_fraction</th>\n",
       "      <th>n_clusters_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S07-hw-dataset-01</td>\n",
       "      <td>KMeans</td>\n",
       "      <td>{'k': 2, 'random_state': 42, 'n_init': 20}</td>\n",
       "      <td>0.521640</td>\n",
       "      <td>0.685330</td>\n",
       "      <td>11786.954623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S07-hw-dataset-02</td>\n",
       "      <td>DBSCAN</td>\n",
       "      <td>{'eps': 0.096, 'min_samples': 12}</td>\n",
       "      <td>0.743035</td>\n",
       "      <td>0.354264</td>\n",
       "      <td>427.534217</td>\n",
       "      <td>0.994875</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S07-hw-dataset-03</td>\n",
       "      <td>KMeans</td>\n",
       "      <td>{'k': 3, 'random_state': 42, 'n_init': 20}</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>1.157726</td>\n",
       "      <td>6957.162640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataset  method                                      params  \\\n",
       "0  S07-hw-dataset-01  KMeans  {'k': 2, 'random_state': 42, 'n_init': 20}   \n",
       "1  S07-hw-dataset-02  DBSCAN           {'eps': 0.096, 'min_samples': 12}   \n",
       "2  S07-hw-dataset-03  KMeans  {'k': 3, 'random_state': 42, 'n_init': 20}   \n",
       "\n",
       "   silhouette  davies_bouldin  calinski_harabasz  noise_fraction  \\\n",
       "0    0.521640        0.685330       11786.954623        0.000000   \n",
       "1    0.743035        0.354264         427.534217        0.994875   \n",
       "2    0.315545        1.157726        6957.162640        0.000000   \n",
       "\n",
       "   n_clusters_eval  \n",
       "0                2  \n",
       "1                3  \n",
       "2                3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for ds_key, cfg in best_configs.items():\n",
    "    m = cfg['metrics']\n",
    "    rows.append({\n",
    "        'dataset': ds_key,\n",
    "        'method': cfg['chosen_method'],\n",
    "        'params': cfg['params'],\n",
    "        'silhouette': m['silhouette'],\n",
    "        'davies_bouldin': m['davies_bouldin'],\n",
    "        'calinski_harabasz': m['calinski_harabasz'],\n",
    "        'noise_fraction': m['noise_fraction'],\n",
    "        'n_clusters_eval': m['n_clusters_eval'],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows).sort_values('dataset')\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
